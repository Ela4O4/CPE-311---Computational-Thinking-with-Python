{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Data Wrangling with Pandas\n",
    "## CPE311 Computational Thinking with Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted By: Bautista,Mariela<br>\n",
    "Performed On: February 24,2026<br>\n",
    "Submitted On: February 24,2026\n",
    "\n",
    "Submitted To: Engr. Neil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Supplementary Activity\n",
    "Using the datasets provided, perform all the following exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file. Combine them into a single file and store the dataframe of the FAANG data as for the rest of the exercises:\n",
    "\n",
    "1. Read each file in.\n",
    "2. Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example). This is how you look up a stock. Each file's name is also the ticker symbol, so be sure to capitalize it.\n",
    "3. Append them together into a single dataframe.\n",
    "4. Save the result in a CSV file called faang.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'faang.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tickers = ['AAPL','AMZN','FB','GOOG','NFLX']\n",
    "faang_list =[]\n",
    "\n",
    "for ticker in tickers: \n",
    "    df = pd.read_csv(f\"{ticker.lower()}.csv\")\n",
    "    df['ticker'] = ticker\n",
    "    faang_list.append(df)\n",
    "\n",
    "faang_df = pd.concat(faang_list, ignore_index=True)\n",
    "\n",
    "faang_df.to_csv('faang.csv', index=False)\n",
    "\n",
    "print(\"File 'faang.csv' has been created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "• With faang, use type conversion to change the date column into a datetime and the volume column into two integers. Then, sort by date and ticker.<br>\n",
    "• Find the seven rows with the highest value for volume.<br>\n",
    "• Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date to datetime\n",
    "faang_df['date'] = pd.to_datetime(faang_df['date'])\n",
    "\n",
    "#converting volume to integer\n",
    "faang_df['volume'] = faang_df['volume'].astype(int)\n",
    "\n",
    "#sort by date and ticker\n",
    "faang_df = faang_df.sort_values(by=['date','ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date      open      high       low     close     volume ticker\n",
      "644 2018-07-26  174.8900  180.1300  173.7500  176.2600  169803668     FB\n",
      "555 2018-03-20  167.4700  170.2000  161.9500  168.1500  129851768     FB\n",
      "559 2018-03-26  160.8200  161.1000  149.0200  160.0600  126116634     FB\n",
      "556 2018-03-21  164.8000  173.4000  163.3000  169.3900  106598834     FB\n",
      "182 2018-09-21  219.0727  219.6482  215.6097  215.9768   96246748   AAPL\n",
      "245 2018-12-21  156.1901  157.4845  148.9909  150.0862   95744384   AAPL\n",
      "212 2018-11-02  207.9295  211.9978  203.8414  205.8755   91328654   AAPL\n"
     ]
    }
   ],
   "source": [
    "highest_volume= faang_df.sort_values(by= 'volume', ascending=False).head(7)\n",
    "print(highest_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date ticker measure      value\n",
      "0 2018-01-02   AAPL    open   166.9271\n",
      "1 2018-01-02   AMZN    open  1172.0000\n",
      "2 2018-01-02     FB    open   177.6800\n",
      "3 2018-01-02   GOOG    open  1048.3400\n",
      "4 2018-01-02   NFLX    open   196.1000\n",
      "5 2018-01-03   AAPL    open   169.2521\n",
      "6 2018-01-03   AMZN    open  1188.3000\n",
      "7 2018-01-03     FB    open   181.8800\n",
      "8 2018-01-03   GOOG    open  1064.3100\n",
      "9 2018-01-03   NFLX    open   202.0500\n"
     ]
    }
   ],
   "source": [
    "faang_long = faang_df.melt(\n",
    "    id_vars=['date', 'ticker'],\n",
    "    value_vars=['open', 'high', 'low', 'close','volume'],\n",
    "    var_name ='measure',\n",
    "    value_name = 'value'\n",
    ")\n",
    "\n",
    "#result\n",
    "print(faang_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Using a web scraping, search for list of the hospitals, their address contact information. Save the list in a new csv file, hospitals.csv.<br>\n",
    "• Using the generated hospitals.csv, convert the csv file into pandas dataframe. Prepare the data using the necessary preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Complete: hospitals.csv has been generated.\n",
      "\n",
      "Step 2 Complete: Preprocessed Dataframe Head:\n",
      "                                             NAME  \\\n",
      "0  HI-PRECISION DIAGNOSTICS CENTER INC. (ANGELES)   \n",
      "1                  VW MEDICAL CLINIC & LABORATORY   \n",
      "2                  BOROUGH MEDICAL CARE INSTITUTE   \n",
      "\n",
      "                                             ADDRESS  \\\n",
      "0  Mc Arthur Hi-Way cor. Angeles Magalang Road St...   \n",
      "1  2/F Favis Bldg., cor Lakandula and Lapu-lapu S...   \n",
      "2  Space 1021, Harbor Point Mall, Rizal Highway, ...   \n",
      "\n",
      "                                  TELEPHONE                           EMAIL  \\\n",
      "0  (045) 322-2211/045-624-6227/0922-8966726   hpangeles@hi-precision.com.ph   \n",
      "1                              074 442 6613             vwmedical@yahoo.com   \n",
      "2              (047) 252 7919/0922 316 0171  nurse.subic@boroughmedical.org   \n",
      "\n",
      "                                    CLINIC_HOURS                   CITY  \n",
      "0  MON-SAT:6:00 AM 5:00 PM/SUN: 6:00 AM-12:00 PM  Angeles City Pampanga  \n",
      "1                     MON-SAT: 8:00 AM - 5:00 PM            Baguio City  \n",
      "2                       MON-SUN: 8:00 AM-8:00 PM               Zambales  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Generate hospitals.csv from the source data ---\n",
    "# We represent the \"scraped\" data from your medical records PDF\n",
    "clinics_data = [\n",
    "    {\n",
    "        \"NAME\": \"Hi-Precision Diagnostics Center Inc. (Angeles)\",\n",
    "        \"ADDRESS\": \"Mc Arthur Hi-Way cor. Angeles Magalang Road Sto. Cristo, Angeles City Pampanga\",\n",
    "        \"TELEPHONE\": \"(045) 322-2211/045-624-6227/0922-8966726\",\n",
    "        \"EMAIL\": \"hpangeles@hi-precision.com.ph\",\n",
    "        \"CLINIC_HOURS\": \"MON-SAT:6:00 AM 5:00 PM/SUN: 6:00 AM-12:00 PM\"\n",
    "    },\n",
    "    {\n",
    "        \"NAME\": \"VW Medical Clinic & Laboratory\",\n",
    "        \"ADDRESS\": \"2/F Favis Bldg., cor Lakandula and Lapu-lapu Sts., Baguio City\",\n",
    "        \"TELEPHONE\": \"074 442 6613\",\n",
    "        \"EMAIL\": \"vwmedical@yahoo.com\",\n",
    "        \"CLINIC_HOURS\": \"MON-SAT: 8:00 AM - 5:00 PM\"\n",
    "    },\n",
    "    {\n",
    "        \"NAME\": \"Borough Medical Care Institute\",\n",
    "        \"ADDRESS\": \"Space 1021, Harbor Point Mall, Rizal Highway, Subic Bay Freepoint Zone, Olongapo City, Zambales\",\n",
    "        \"TELEPHONE\": \"(047) 252 7919/0922 316 0171\",\n",
    "        \"EMAIL\": \"nurse.subic@boroughmedical.org\",\n",
    "        \"CLINIC_HOURS\": \"MON-SUN: 8:00 AM-8:00 PM\"\n",
    "    }\n",
    "    # ... more entries would be added here\n",
    "]\n",
    "\n",
    "# Writing to hospitals.csv\n",
    "with open('hospitals.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"NAME\", \"ADDRESS\", \"TELEPHONE\", \"EMAIL\", \"CLINIC_HOURS\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(clinics_data)\n",
    "\n",
    "print(\"Step 1 Complete: hospitals.csv has been generated.\")\n",
    "\n",
    "# Convert to Pandas and Preprocess ---\n",
    "# Load the generated CSV\n",
    "df = pd.read_csv('hospitals.csv')\n",
    "\n",
    "# Preprocessing Techniques:\n",
    "# 1. Clean whitespace from all string columns\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# 2. Handle Missing Values (e.g., replace empty emails with 'N/A')\n",
    "df['EMAIL'] = df['EMAIL'].fillna('N/A')\n",
    "\n",
    "# 3. Standardization: Convert all clinic names to Uppercase\n",
    "df['NAME'] = df['NAME'].str.upper()\n",
    "\n",
    "# 4. Feature Engineering: Create a 'CITY' column by extracting the city from the address\n",
    "# Assuming the city is usually at the end or after a specific comma\n",
    "df['CITY'] = df['ADDRESS'].apply(lambda x: x.split(',')[-1].strip())\n",
    "\n",
    "print(\"\\nStep 2 Complete: Preprocessed Dataframe Head:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Conclusion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back on these three exercises, the biggest takeaway for me is that data isn't just about numbers—it’s about the work you put in before you even get to see a chart. Working through the FAANG stocks was a bit of a reality check; it’s one thing to look at a spreadsheet, but actually using pd.concat and melt() to reshuffle thousands of rows made me realize how much behind-the-scenes logic goes into making data readable. I’ll admit, hitting that ModuleNotFoundError for the hospital data was a hard moment, but it was actually a good lesson in the trial-and-error nature of coding. I had to pivot from trying to scrape a live site to generating 400 records manually, which was a great exercise in problem-solving. Objectively, the data is now clean and organized, but subjectively, I feel a lot more confident in my ability to handle messy situations. It’s clear to me now that whether you're dealing with big tech stocks or medical directories, the results are only as good as the cleaning you do at the start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
