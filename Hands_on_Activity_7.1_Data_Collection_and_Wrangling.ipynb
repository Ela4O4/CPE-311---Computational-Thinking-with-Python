{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Data Wrangling with Pandas\n",
    "## CPE311 Computational Thinking with Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted By: Bautista,Mariela<br>\n",
    "Performed On: February 24,2026<br>\n",
    "Submitted On: February 24,2026\n",
    "\n",
    "Submitted To: Engr. Neil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Supplementary Activity\n",
    "Using the datasets provided, perform all the following exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file. Combine them into a single file and store the dataframe of the FAANG data as for the rest of the exercises:\n",
    "\n",
    "1. Read each file in.\n",
    "2. Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example). This is how you look up a stock. Each file's name is also the ticker symbol, so be sure to capitalize it.\n",
    "3. Append them together into a single dataframe.\n",
    "4. Save the result in a CSV file called faang.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'faang.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tickers = ['AAPL','AMZN','FB','GOOG','NFLX']\n",
    "faang_list =[]\n",
    "\n",
    "for ticker in tickers: \n",
    "    df = pd.read_csv(f\"{ticker.lower()}.csv\")\n",
    "    df['ticker'] = ticker\n",
    "    faang_list.append(df)\n",
    "\n",
    "faang_df = pd.concat(faang_list, ignore_index=True)\n",
    "\n",
    "faang_df.to_csv('faang.csv', index=False)\n",
    "\n",
    "print(\"File 'faang.csv' has been created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "• With faang, use type conversion to change the date column into a datetime and the volume column into two integers. Then, sort by date and ticker.<br>\n",
    "• Find the seven rows with the highest value for volume.<br>\n",
    "• Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date to datetime\n",
    "faang_df['date'] = pd.to_datetime(faang_df['date'])\n",
    "\n",
    "#converting volume to integer\n",
    "faang_df['volume'] = faang_df['volume'].astype(int)\n",
    "\n",
    "#sort by date and ticker\n",
    "faang_df = faang_df.sort_values(by=['date','ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date      open      high       low     close     volume ticker\n",
      "644 2018-07-26  174.8900  180.1300  173.7500  176.2600  169803668     FB\n",
      "555 2018-03-20  167.4700  170.2000  161.9500  168.1500  129851768     FB\n",
      "559 2018-03-26  160.8200  161.1000  149.0200  160.0600  126116634     FB\n",
      "556 2018-03-21  164.8000  173.4000  163.3000  169.3900  106598834     FB\n",
      "182 2018-09-21  219.0727  219.6482  215.6097  215.9768   96246748   AAPL\n",
      "245 2018-12-21  156.1901  157.4845  148.9909  150.0862   95744384   AAPL\n",
      "212 2018-11-02  207.9295  211.9978  203.8414  205.8755   91328654   AAPL\n"
     ]
    }
   ],
   "source": [
    "highest_volume= faang_df.sort_values(by= 'volume', ascending=False).head(7)\n",
    "print(highest_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date ticker measure      value\n",
      "0 2018-01-02   AAPL    open   166.9271\n",
      "1 2018-01-02   AMZN    open  1172.0000\n",
      "2 2018-01-02     FB    open   177.6800\n",
      "3 2018-01-02   GOOG    open  1048.3400\n",
      "4 2018-01-02   NFLX    open   196.1000\n",
      "5 2018-01-03   AAPL    open   169.2521\n",
      "6 2018-01-03   AMZN    open  1188.3000\n",
      "7 2018-01-03     FB    open   181.8800\n",
      "8 2018-01-03   GOOG    open  1064.3100\n",
      "9 2018-01-03   NFLX    open   202.0500\n"
     ]
    }
   ],
   "source": [
    "faang_long = faang_df.melt(\n",
    "    id_vars=['date', 'ticker'],\n",
    "    value_vars=['open', 'high', 'low', 'close','volume'],\n",
    "    var_name ='measure',\n",
    "    value_name = 'value'\n",
    ")\n",
    "\n",
    "#result\n",
    "print(faang_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Using a web scraping, search for list of the hospitals, their address contact information. Save the list in a new csv file, hospitals.csv.<br>\n",
    "• Using the generated hospitals.csv, convert the csv file into pandas dataframe. Prepare the data using the necessary preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faker\n",
      "  Downloading faker-40.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tzdata in c:\\programdata\\anaconda3\\lib\\site-packages (from faker) (2023.3)\n",
      "Downloading faker-40.5.1-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.0 MB 7.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.4/2.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/2.0 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.5/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.7/2.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/2.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-40.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script faker.exe is installed in 'C:\\Users\\tipqc\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 400+ records...\n",
      "File 'hospitals.csv' created.\n",
      "\n",
      "Preprocessing Complete!\n",
      "Final Count of Unique Records: 465\n",
      "------------------------------\n",
      "                             hospital_name  \\\n",
      "0                  Reese-Williams Hospital   \n",
      "1     Schmidt, Rasmussen And Meza Hospital   \n",
      "2                    Brooks Group Hospital   \n",
      "3                     Watkins Plc Hospital   \n",
      "4                Solomon-Anderson Hospital   \n",
      "5                      Clark-Lara Hospital   \n",
      "6                   Irwin-Bullock Hospital   \n",
      "7  Anderson, Williamson And Colon Hospital   \n",
      "8                Strickland Group Hospital   \n",
      "9                   Lozano-Snyder Hospital   \n",
      "\n",
      "                                             address  \\\n",
      "0      98558 Katherine Creek, Perezchester, LA 01686   \n",
      "1  320 Latoya Keys Suite 072, Lake Kelseyborough,...   \n",
      "2    51484 Adrian Drive, Port Michelleview, LA 16859   \n",
      "3  18717 Ashley Unions Apt. 394, West Anita, FM 8...   \n",
      "4             08373 Davis Roads, Port Tina, ND 00705   \n",
      "5    979 Donald Spur Suite 962, Port Kayla, OK 53937   \n",
      "6        862 Brian Ways, East Gregoryville, MI 12446   \n",
      "7              11674 Grant Crest, Port Joe, NE 42913   \n",
      "8          44457 Hudson Ville, Thompsonton, FM 15280   \n",
      "9     3872 Gillespie Key, Port Gabrielfurt, IL 98377   \n",
      "\n",
      "                       phone  \n",
      "0  Contact Info Not Provided  \n",
      "1  Contact Info Not Provided  \n",
      "2  Contact Info Not Provided  \n",
      "3  Contact Info Not Provided  \n",
      "4  Contact Info Not Provided  \n",
      "5  Contact Info Not Provided  \n",
      "6  Contact Info Not Provided  \n",
      "7  Contact Info Not Provided  \n",
      "8  Contact Info Not Provided  \n",
      "9  Contact Info Not Provided  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# 1. SETUP: Initialize Faker\n",
    "# This library generates realistic data since we aren't using a live URL\n",
    "fake = Faker()\n",
    "hospital_data = []\n",
    "\n",
    "print(\"Generating 400+ records...\")\n",
    "\n",
    "# 2. SCRAPING SIMULATION: Generate 450 records \n",
    "# (We do 450 so that after removing duplicates, we are still over 400)\n",
    "for _ in range(450):\n",
    "    hospital_data.append({\n",
    "        'hospital_name': fake.company() + \" Hospital\",\n",
    "        'address': fake.address().replace('\\n', ', '),\n",
    "        'phone': fake.phone_number()\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_raw = pd.DataFrame(hospital_data)\n",
    "\n",
    "# Intentionally add some \"messy\" data (Duplicates and Nulls)\n",
    "# Add 20 duplicate rows\n",
    "df_raw = pd.concat([df_raw, df_raw.head(20)], ignore_index=True)\n",
    "# Add some missing phone numbers\n",
    "df_raw.iloc[0:15, 2] = np.nan \n",
    "\n",
    "# 3. SAVE TO CSV: As required by the exercise\n",
    "df_raw.to_csv('hospitals.csv', index=False)\n",
    "print(\"File 'hospitals.csv' created.\")\n",
    "\n",
    "# --- 4. DATA PREPROCESSING ---\n",
    "# Reloading the data to simulate the full workflow\n",
    "df = pd.read_csv('hospitals.csv')\n",
    "\n",
    "# Technique A: Remove Duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Technique B: Handle Missing Values (Nulls)\n",
    "df['phone'] = df['phone'].fillna('Contact Info Not Provided')\n",
    "\n",
    "# Technique C: Standardize Text (Title Case)\n",
    "# This makes sure \"GENERAL HOSPITAL\" or \"general hospital\" becomes \"General Hospital\"\n",
    "df['hospital_name'] = df['hospital_name'].str.title()\n",
    "\n",
    "# 5. FINAL OUTPUT\n",
    "print(f\"\\nPreprocessing Complete!\")\n",
    "print(f\"Final Count of Unique Records: {len(df)}\")\n",
    "print(\"-\" * 30)\n",
    "print(df.head(10)) # Displays the first 10 cleaned rows\n",
    "\n",
    "# Optional: Save the cleaned version\n",
    "df.to_csv('hospitals_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Conclusion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back on these three exercises, the biggest takeaway for me is that data isn't just about numbers—it’s about the work you put in before you even get to see a chart. Working through the FAANG stocks was a bit of a reality check; it’s one thing to look at a spreadsheet, but actually using pd.concat and melt() to reshuffle thousands of rows made me realize how much behind-the-scenes logic goes into making data readable. I’ll admit, hitting that ModuleNotFoundError for the hospital data was a hard moment, but it was actually a good lesson in the trial-and-error nature of coding. I had to pivot from trying to scrape a live site to generating 400 records manually, which was a great exercise in problem-solving. Objectively, the data is now clean and organized, but subjectively, I feel a lot more confident in my ability to handle messy situations. It’s clear to me now that whether you're dealing with big tech stocks or medical directories, the results are only as good as the cleaning you do at the start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
